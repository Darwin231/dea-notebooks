{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be66eaa4-9197-47bc-976d-f19149b7af79",
   "metadata": {},
   "source": [
    "# Prototype Aus-wide saltmarsh\n",
    "\n",
    "## Conceptual workflow \n",
    "\n",
    "The covariate data will be flexible, starting with Landsat derived metrics\n",
    "\n",
    "For the moment, to develop performance, use a once-off geomoedian to represent an agreed time period. Something like:  \n",
    "    \n",
    "    1. Decide on a time period, a 2 to 3 year period (2019-2021? - if we stick to post-2015 we can integrate Sentinel-2 down the track more easily)  \n",
    "    2. Sample from the whole Australia intertidal data set to get wetland vs. not wetland [but for the moment skip to saltmarsh vs land/water?]  \n",
    "    3. Fit a ML model based on ALL the Aus-wide data (e.g. RF or SVM etc.)  \n",
    "    4. Predict out to some known locations (eventually guided runs across the whole coastline)\n",
    "    5. Explore and validate  \n",
    "\n",
    "\n",
    "***Cummulaive Q's:***\n",
    "- RE the `query` dictionary passed to the loading function or loading helper: does the `output_crs` only apply to output after the processing subsquent to the load, or does it always apply to every time-step found in the collection?  \n",
    "- what happens when you stop execution while dask chunks are processing/queued (seems not to like that...)?  \n",
    "- for lazy eval, is it quicker to `.load()`/`.compute()` just the individual bands, several at one, or the whole lot?  And if several at once, how to do that via the slot method (i.e. `lazy_dat.blue.load()` --> `lazy_day.some_bands.load()`  \n",
    "- where does the datacube connection need to go? functionalised as needed, or just up top??\n",
    "- what EPSG t owork with in general for various Aus-wide prediciton windows?\n",
    "- what does the `dc = datacube.Datacube(app='Classify_satellite_data')` call actually do?\n",
    "- what is the current standard for pixel-based cloud filtering (for LS8 and S2)\n",
    "\n",
    "\n",
    "** critical Q's **\n",
    "- once settled on a larger area, should we export the temporal composites/metrics?\n",
    "- is it possible to do a lazy training sample (like for a geo-median) but for other computed metrics? (i.e. do they need to be pre-computed, or can they also be only computed where a trianing point is). THe reason being is that we can fit and save the FULL model, but only predict to where we need\n",
    "- prefer to make multiple scripts that generate progressive outputs (e.g. layer metrics, fitted model, intermediate classifications) OR use flags iwthin one script to skip and load if already done?\n",
    "- how do i get the training data sampling process to keep more than one column? (i.e. to add further covariates or class labels, or to say keep an ID column to enable join-merge after)\n",
    "- how to get medians for dask array? how do you usually do geomedians - explicitly with a dedicated function?\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc195e5f-19cf-4a28-8b7e-5f0e3a3206fc",
   "metadata": {},
   "source": [
    "## Load packages/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56287530-af06-47e1-870a-aacb08473678",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import subprocess as sp\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from datacube.utils.geometry import assign_crs\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../Tools/')\n",
    "from dea_tools.datahandling import load_ard\n",
    "from dea_tools.plotting import map_shapefile, rgb, display_map\n",
    "from dea_tools.bandindices import calculate_indices\n",
    "from dea_tools.classification import collect_training_data\n",
    "from dea_tools.classification import predict_xr\n",
    "from dea_tools.dask import create_local_dask_cluster\n",
    "\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, balanced_accuracy_score\n",
    "\n",
    "import dask.array as da\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# connect to the datacube\n",
    "dc = datacube.Datacube(app='aust_saltmarsh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5d811-655d-4d4f-805f-e25d299b2ec6",
   "metadata": {},
   "source": [
    "### get ncpus to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099bf2e7-5665-4576-9398-1a6395eab25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 2\n"
     ]
    }
   ],
   "source": [
    "ncpus = round(get_cpu_quota())\n",
    "print('ncpus = ' + str(ncpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e0362d-42bc-41e8-9eba-ec240fbaf099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.0\n"
     ]
    }
   ],
   "source": [
    "print(xr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153589b-db11-4d86-8862-d932b808688f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
