{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Fractional Cover to the reprocessed FC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022 Update:\n",
    "We are running this notebook to check that the reprocessed FC in 02/2022 will improve the alignment of Landsat 8 FC with Landsat 5 and 7 FC. Discrepancies were observed after the first full DEA Collection 3 Landsat Vegetation Fractional Cover processing.\n",
    "\n",
    "\n",
    "- This requires us to recalculate the FC to demonstrate the fix\n",
    "\n",
    "- This notebook will ideally demonstrate that the reprocessed FC aligns better with the field data and with LS 5 and LS 7 FC than the original FC C3 data did. \n",
    "\n",
    "- For reprocessed FC, take coefficients from here: https://github.com/GeoscienceAustralia/dea-config/blob/master/prod/services/alchemist/ga_ls_fc_3/ga_ls_fc_3.alchemist.yaml \n",
    "\n",
    "- the updated FC coefficients are applied after the first FC calculation. The first FC calculation is performed using the existing FC module. band * scale + interception will be good enough, e.g. bs * 0.9499 + 2.45 \n",
    "\n",
    " See https://github.com/GeoscienceAustralia/fc/pull/48/files\n",
    " fc_coefficients:\n",
    " \n",
    " bs:\n",
    " - 2.45\n",
    "- 0.9499\n",
    "\n",
    "pv:\n",
    "- 2.77\n",
    "- 0.9481\n",
    "\n",
    "npv:\n",
    "- -0.73\n",
    "- 0.9578 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find field data; this field data is the Star transects from [the JRSRP geoserver wfs service](https://field-geoserver.jrsrp.com/geoserver/aus/wfs?service=wfs&version=1.1.0&request=GetFeature&typeNames=aus:star_transects&outputFormat=csv) which can be visualised through [the TERN Landscapes-JRSRP Field Data Portal](https://field.jrsrp.com/) and is available as a csv\n",
    "- Load corresponding surface reflectance from datacube or save file\n",
    "- Calculate FC and compare to field data using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install additional packages before running this notebook the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FC repository is used in this notebook to calculate DEA Fractional Cover. \n",
    "- I cloned the fractional cover repository https://github.com/GeoscienceAustralia/fc into ~/dev/fc on 07/02/2022 before PR #48 implemented (Emma's fix of ls8 fc coefficients to deal with changes in the Collection Upgrade of DEA data from Collection 2 to Collection 3 Landsat data).\n",
    "- To install the fractional cover module, I ran `#!pip install --extra-index-url=https://packages.dea.ga.gov.au/ fc` on 07/02/2022 (prior to PR #48 being approved)\n",
    "- If you have not installed the fractional cover module, you will need to do so before running this notebook. This can be completed either by cloning the FC module as I did above or by running the command in the cell below, as I use in this notebook following instructions in the `~/dev/fc/README.rst` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --extra-index-url=https://packages.dea.ga.gov.au/ fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install dea-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "\n",
    "To run this analysis, choose the satellite you are comparing to the field data in the next cell by uncommenting its name \n",
    "e.g. `sensor_name = 'Landsat 8'`\n",
    "Then run all the cells in the notebook, starting with the \"Define sensor name\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose which sensor we are comparing to the field data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sensor name\n",
    "Choose which satellite to compare to field data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_name = 'Landsat 8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import datacube\n",
    "from fc.fractional_cover import compute_fractions\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import sys\n",
    "\n",
    "#import dea-tools module - this should have been pip-installed earlier\n",
    "#sys.path.insert(1, '../../../Tools/') #this may need moving later\n",
    "from dea_tools.datahandling import load_ard\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec \n",
    "from shapely import wkt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "# instantiate a datacube\n",
    "dc = datacube.Datacube()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to compute the fractional covers as viewed by the satellite for the site\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the fractional covers as viewed by the satellite for the site\n",
    "# Required a site properties object\n",
    "\n",
    "def fractionalCoverSatView(siteProperties):\n",
    "    '''equations to calculate fractional cover from the csv data'''\n",
    "    nTotal = siteProperties['num_points']\n",
    "    \n",
    "    # Canopy Layer\n",
    "    nCanopyBranch = siteProperties['over_b'] * nTotal / 100.0\n",
    "    nCanopyDead = siteProperties['over_d'] * nTotal / 100.0\n",
    "    nCanopyGreen = siteProperties['over_g'] * nTotal / 100.0\n",
    "    \n",
    "    # Midstory Layer\n",
    "    nMidBranch = siteProperties['mid_b'] * nTotal / 100.0\n",
    "    nMidGreen = siteProperties['mid_g'] * nTotal / 100.0\n",
    "    nMidDead = siteProperties['mid_d'] * nTotal / 100.0\n",
    "    \n",
    "    # Ground Layer\n",
    "    nGroundDeadLitter = (siteProperties['dead'] + siteProperties['litter']) * nTotal / 100.0\n",
    "    nGroundCrustDistRock = (siteProperties['crust'] + siteProperties['dist'] + siteProperties['rock']) * nTotal / 100.0\n",
    "    nGroundGreen = siteProperties['green'] * nTotal / 100.0\n",
    "    nGroundCrypto = siteProperties['crypto'] * nTotal / 100.0\n",
    "    \n",
    "    # Work out the canopy elements as viewed from above\n",
    "    canopyFoliageProjectiveCover = nCanopyGreen / (nTotal - nCanopyBranch)\n",
    "    canopyDeadProjectiveCover = nCanopyDead / (nTotal - nCanopyBranch)\n",
    "    canopyBranchProjectiveCover = nCanopyBranch / nTotal * (1.0 - canopyFoliageProjectiveCover - canopyDeadProjectiveCover)\n",
    "    canopyPlantProjectiveCover = (nCanopyGreen+nCanopyDead + nCanopyBranch) / nTotal\n",
    "    \n",
    "    # Work out the midstorey fractions\n",
    "    midFoliageProjectiveCover = nMidGreen / nTotal\n",
    "    midDeadProjectiveCover = nMidDead / nTotal\n",
    "    midBranchProjectiveCover = nMidBranch / nTotal\n",
    "    midPlantProjectiveCover = (nMidGreen + nMidDead + nMidBranch) / nTotal\n",
    "    \n",
    "    # Work out the midstorey  elements as viewed by the satellite using a gap fraction method\n",
    "    satMidFoliageProjectiveCover = midFoliageProjectiveCover * (1 - canopyPlantProjectiveCover)\n",
    "    satMidDeadProjectiveCover = midDeadProjectiveCover * (1 - canopyPlantProjectiveCover)\n",
    "    satMidBranchProjectiveCover = midBranchProjectiveCover * (1 - canopyPlantProjectiveCover)\n",
    "    satMidPlantProjectiveCover = midPlantProjectiveCover * (1 - canopyPlantProjectiveCover)\n",
    "    \n",
    "    # Work out the groundcover fractions as seen by the observer\n",
    "    groundPVCover = nGroundGreen / nTotal\n",
    "    groundNPVCover = nGroundDeadLitter / nTotal\n",
    "    groundBareCover = nGroundCrustDistRock / nTotal\n",
    "    groundCryptoCover = nGroundCrypto / nTotal\n",
    "    groundTotalCover = (nGroundGreen + nGroundDeadLitter + nGroundCrustDistRock) / nTotal\n",
    "    \n",
    "    # Work out the ground cover proportions as seen by the satellite\n",
    "    satGroundPVCover = groundPVCover * (1 - midPlantProjectiveCover) * (1 - canopyPlantProjectiveCover)\n",
    "    satGroundNPVCover = groundNPVCover * ( 1- midPlantProjectiveCover) * (1 - canopyPlantProjectiveCover)\n",
    "    satGroundBareCover = groundBareCover * (1 - midPlantProjectiveCover) * (1 - canopyPlantProjectiveCover)\n",
    "    satGroundCryptoCover = groundCryptoCover * (1 - midPlantProjectiveCover) * (1 - canopyPlantProjectiveCover)\n",
    "    satGroundTotalCover = groundTotalCover * (1 - midPlantProjectiveCover) * (1 - canopyPlantProjectiveCover)\n",
    "    \n",
    "    # Final total covers calculated using gap probabilities through all layers\n",
    "    totalPVCover = canopyFoliageProjectiveCover + satMidFoliageProjectiveCover + satGroundPVCover\n",
    "    totalNPVCover = canopyDeadProjectiveCover + canopyBranchProjectiveCover + satMidDeadProjectiveCover + satMidBranchProjectiveCover + satGroundNPVCover\n",
    "    totalBareCover = satGroundBareCover\n",
    "    totalCryptoCover = satGroundCryptoCover\n",
    "    \n",
    "    return np.array([totalPVCover,totalNPVCover+totalCryptoCover,totalBareCover])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions coefficients for FC computation\n",
    "These are the `Landsat 8 Fudge Factor` used to make the spectrally different sensor on Landsat 8 perform similarly to Landsats 5 and 7 in the Fractional Cover algorithm. These are incorporated in to the Collection 3 config as `regression_coefficients` in [C3 product definition yaml](https://github.com/opendatacube/datacube-alchemist/blob/C3_Processing/examples/c3_config_fc.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients for FC compute\n",
    "\n",
    "#these were used in collection 2 but are not correct as don't multiply by 10000/ 1e4\n",
    "ls8_coefficients_c2 = {'blue': [0.00041, 0.9747], 'green': [0.00289, 0.99779], 'red': [0.00274, 1.00446], \n",
    "                       'nir': [4e-05, 0.98906], 'swir1': [0.00256, 0.99467], 'swir2': [-0.00327, 1.02551]}\n",
    "\n",
    "#use these ones, they are correct (need to multiply by 10000 as the algorithm expects 0-10000 not 0-1)\n",
    "ls8_coefficients = {'blue': [0.00041*1e4, 0.9747], 'green': [0.00289*1e4, 0.99779], 'red': [0.00274*1e4, 1.00446], \n",
    "                       'nir': [4e-05*1e4, 0.98906], 'swir1': [0.00256*1e4, 0.99467], 'swir2': [-0.00327*1e4, 1.02551]}\n",
    "\n",
    "s2_coefficients = {'blue':[-0.0022*1e4, 0.9551],\n",
    "                   'green':[0.0031*1e4, 1.0582],\n",
    "                   'red':[0.0064*1e4, 0.9871],\n",
    "                   'nir':[0.012*1e4, 1.0187],\n",
    "                   'swir1':[0.0079*1e4, 0.9528],\n",
    "                   'swir2':[-0.0042*1e4, 0.9688]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3 Reprocessing coefficients for updated FC C3 computation for Landsat 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fc coefficients updated as per https://github.com/GeoscienceAustralia/dea-config/blob/master/prod/services/alchemist/ga_ls_fc_3/ga_ls_fc_3.alchemist.yaml\n",
    "fc_coefficients = {'bs':[2.45, 0.9499],\n",
    "                   'pv':[2.77, 0.9481],\n",
    "                   'npv':[-0.73, 0.9578]}                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute FC \n",
    "# def compute_fc(input_ds, regression_coefficients):\n",
    "#     '''takes input dataset and multiplies by regression coefficients to compute fractional cover.\n",
    "#     returns an xarray DataArray with pv,npv,bs,ue bands'''\n",
    "    \n",
    "#     input_data = input_ds.to_array().data\n",
    "#     is_valid_array= (input_data >0).all(axis=0)\n",
    "#     # Set nodata to 0                                                       \n",
    "#     input_data[:, ~is_valid_array] = 0\n",
    "#     # compute fractional_cover\n",
    "#     output_data = compute_fractions(input_data, regression_coefficients)\n",
    "#     output_data[:, ~is_valid_array] = -1\n",
    "#     return xr.DataArray(output_data, dims=('band','y','x'),\n",
    "#                         coords={'x':input_ds.x, 'y':input_ds.y, 'band':['pv', 'npv', 'bs', 'ue']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute FC - updated with new fc_coefficients\n",
    "def compute_fc(input_ds, regression_coefficients):#, fc_coefficients):\n",
    "    '''takes input dataset and multiplies by regression coefficients to compute fractional cover.\n",
    "    returns an xarray DataArray with pv,npv,bs,ue bands'''\n",
    "    \n",
    "    input_data = input_ds.to_array().data\n",
    "    is_valid_array= (input_data >0).all(axis=0)\n",
    "    # Set nodata to 0                                                       \n",
    "    input_data[:, ~is_valid_array] = 0\n",
    "    # compute fractional_cover\n",
    "    output_data = compute_fractions(input_data, regression_coefficients)\n",
    "    output_data[:, ~is_valid_array] = -1\n",
    "    print(output_data)\n",
    "    return xr.DataArray(output_data, dims=('band','y','x'),\n",
    "                        coords={'x':input_ds.x, 'y':input_ds.y, 'band':['pv', 'npv', 'bs', 'ue']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select good pixels using pixel quality\n",
    "def ls_good(pq):\n",
    "    return masking.make_mask(pq, cloud_acca = \"no_cloud\", cloud_fmask = \"no_cloud\",\n",
    "                             cloud_shadow_acca = \"no_cloud_shadow\",\n",
    "                             cloud_shadow_fmask = \"no_cloud_shadow\",\n",
    "                             contiguous = True)\n",
    "def s2_good(pq):\n",
    "    return pq == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the query for each sensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral bands used for fractional cover calculation\n",
    "\n",
    "#this is correct for collection3  \n",
    "ls_bands = ['nbart_green','nbart_red','nbart_nir','nbart_swir_1','nbart_swir_2']\n",
    "\n",
    "#this is correct for collection 3\n",
    "s2_bands = ['nbart_green','nbart_red','nbart_nir_1','nbart_swir_2','nbart_swir_3']\n",
    "\n",
    "# sensor specific configurations - note no Landsat 5 as doesn't overlap with site surveys\n",
    "\n",
    "sensor_config = {'Landsat 7':{'startdate':'1999-05-01', #updated for collection3\n",
    "                              'product':'ga_ls7e_ard_3', \n",
    "                              'bands':ls_bands,\n",
    "                              'resolution':(-30,30), \n",
    "                              'fc_coefficients': None}, \n",
    "                 \n",
    "                 'Landsat 8 noscaling':{'startdate':'2013-03-01',\n",
    "                                        'product':'ga_ls8c_ard_3', \n",
    "                                        'bands':ls_bands,\n",
    "                                        'resolution':(-30,30), \n",
    "                                        'fc_coefficients': None}, \n",
    "                 \n",
    "                 'Landsat 8':{'startdate':'2013-03-01', \n",
    "                              'product':'ga_ls8c_ard_3', \n",
    "                              'bands':ls_bands,\n",
    "                              'resolution':(-30,30), \n",
    "                              'fc_coefficients':ls8_coefficients}, \n",
    "                 \n",
    "                 'Landsat 8 c2':{'startdate':'2013-03-01', \n",
    "                                      'product':'ga_ls8c_ard_3', \n",
    "                                      'bands':ls_bands,\n",
    "                                      'resolution':(-30,30), \n",
    "                                      'fc_coefficients':ls8_coefficients_c2}, \n",
    "                 \n",
    "                 'Sentinel 2A':{'startdate':'2015-07-01', \n",
    "                                'product':'s2a_ard_granule', \n",
    "                                'bands':s2_bands,\n",
    "                                'resolution':(-10,10), \n",
    "                                'fc_coefficients':s2_coefficients},\n",
    "                 \n",
    "                'Sentinel 2B':{'startdate':'2017-06-01', \n",
    "                               'product':'s2b_ard_granule', \n",
    "                               'bands':s2_bands,\n",
    "                               'resolution':(-10,10), \n",
    "                               'fc_coefficients':s2_coefficients}\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load field data in from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load star_transects field data \n",
    "field = pd.read_csv('star_transects_short.csv')\n",
    "field['geometry'] = field.geom.apply(wkt.loads)\n",
    "field = gpd.GeoDataFrame(field)\n",
    "\n",
    "#field data comes in in WGS84\n",
    "field.crs = {'init': 'EPSG:4326'}\n",
    "\n",
    "#transform to Australian Albers Equal Area \n",
    "field = field.to_crs({'init':'EPSG:3577'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data by date - get dates later than the first observation of the satellite\n",
    "field = field.loc[field['obs_time'] > sensor_config[sensor_name]['startdate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate field measured fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate field measured fractions\n",
    "field = field.merge(\n",
    "    field.apply(fractionalCoverSatView, axis=1, result_type= 'expand').rename(\n",
    "        columns = {0:'total_pv',1:'total_npv',2:'total_bs'}),\n",
    "    left_index=True, right_index=True)\n",
    "field = field[field.apply(lambda x: x['total_pv']+x['total_npv']+x['total_bs'], axis=1) >0.95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match to albers tiles to check distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match to albers tiles to check distribution\n",
    "albers_tiles = gpd.read_file('../Validation_against_field_data/Albers_Australia_Coast_and_Islands.shp')\n",
    "albers_tiles.crs = {'init':'EPSG:3577'}\n",
    "matched = gpd.sjoin(field, albers_tiles, how='inner', op = 'intersects')\n",
    "field_tiles = albers_tiles.merge(matched.groupby('label')['FID'].count().sort_values(ascending=False).to_frame('count').reset_index()\n",
    "    , on='label', how='right')\n",
    "print(\"Total number of data points is\",len(field))\n",
    "print(\"Largest number of data points in a tile is\", field_tiles.loc[field_tiles['count'].idxmax()]['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual check of field data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check of field data distribution\n",
    "f, axes = plt.subplots(1, 2, figsize=(10,3))\n",
    "plt.suptitle('Density of field data (star transects per albers tile)', size =14)\n",
    "ax0 = field.plot(markersize=1, ax=axes[0])\n",
    "albers_tiles.plot(alpha=0.2, ax = ax0)\n",
    "levels = list(range(0,300,100))\n",
    "ax1 =field_tiles.plot(column='count', cmap = 'viridis', ax=axes[1], legend=True)\n",
    "albers_tiles.plot(alpha=0.2, ax = ax1)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate fractional cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fractional cover from surface reflectance\n",
    "def fractionalCover(row, sensor_config = sensor_config[sensor_name], plot_rad = 50, window_days = 15):\n",
    "    '''this finds data within 15 days of an observation and 50 ??? of an observation'''\n",
    "\n",
    "    # nodata default to return in case no data matches the chosen window around the field observation\n",
    "    fc_dict = {'fc_time': '', 'pv': -1, 'npv': -1, 'bs': -1, 'pv_std': -1, 'npv_std': -1, 'bs_std': -1 }\n",
    "\n",
    "    # define search - grab near observations in space and time\n",
    "    x = row.geometry.x - plot_rad, row.geometry.x + plot_rad\n",
    "    y = row.geometry.y - plot_rad, row.geometry.y + plot_rad\n",
    "    time = (str(np.datetime64(row.obs_time) - np.timedelta64(window_days,'D')),\n",
    "            str(np.datetime64(row.obs_time) + np.timedelta64(window_days,'D'))\n",
    "           )\n",
    "\n",
    "    #use a reusable query dictionary\n",
    "    query = {'measurements':sensor_config['bands'],\n",
    "             'group_by':'solar_day',\n",
    "             'x' : x, \n",
    "             'y' : y,  \n",
    "             'time' : time, \n",
    "             'crs' : 'EPSG:3577', #this defines the crs of the input query\n",
    "             'resolution' : sensor_config['resolution'],\n",
    "             'output_crs' : 'EPSG:3577'}          \n",
    "\n",
    "    try: \n",
    "        #need to test if load_ard will return data here, as the alternative is that the data is not returned\n",
    "        nbart = load_ard(dc,\n",
    "            products = [sensor_config['product']], #need square brackets to deliver as a list\n",
    "            min_gooddata=1, #need all valid pixels for the comparison\n",
    "            fmask_categories=['valid'], #don't want snow or water in fractional cover comparison\n",
    "            mask_pixel_quality=True, #this could give us some issues with dtype later on\n",
    "            mask_contiguity='nbart_contiguity',\n",
    "            ls7_slc_off=True,\n",
    "            **query); #suppress output for this function to save printing space\n",
    "        \n",
    "    except ValueError:\n",
    "        print(f\"No data found at {x},{y},{time}\")\n",
    "        return fc_dict\n",
    "    \n",
    "    # If there aren't any results, this function will return nodata values.\n",
    "    if len(nbart.time) == 0: return fc_dict\n",
    "\n",
    "    ### choose the closest clear timestep to keep\n",
    "\n",
    "    # only keep closest time\n",
    "    nbart = nbart.isel(time=[np.abs(nbart.time-np.datetime64(row.obs_time)).argmin()])\n",
    "\n",
    "    # compute FC\n",
    "    fc = nbart.groupby('time').apply(compute_fc,\n",
    "                                     regression_coefficients = sensor_config['fc_coefficients'],\n",
    "                                    ).to_dataset(dim='band')\n",
    "\n",
    "        # take average\n",
    "    fc_mean = fc.where(fc>=0).groupby('time').mean(dim=['x','y'])\n",
    "    fc_std = fc.where(fc>=0).groupby('time').std(dim=['x','y'])\n",
    "\n",
    "    fc_dict['fc_time'] = fc.time.values[0].astype(str)\n",
    "    for var_name in fc_mean.data_vars:\n",
    "        fc_dict[var_name.lower()] = fc_mean[var_name].values[0]\n",
    "        fc_dict[var_name.lower()+'_std'] = fc_std[var_name].values[0]\n",
    "        \n",
    "    return fc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Fractional Cover satellite data that matches the field site data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# If there aren't any results, this function will return nodata values.\n",
    "fractions = field.apply(fractionalCover, axis=1, result_type = 'expand')\n",
    "field = field.merge(fractions, how = 'inner', left_index=True, right_index=True)\n",
    "field = field[field['pv']>=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "field.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#field.to_file('field_full_with_fc_%s.shp'%''.join(sensor_name.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#field = gpd.read_file('field_full_with_fc_%s.shp'%''.join(sensor_name.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regress "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  `field` is a massive geodataframe full of results which we can write to a shapefile to preserve our results\n",
    "Within this GeoDataFrame, the columns `bs, pv, npv, pv_std, npv_std, bs_std, ue, ue_std` are calculated from surface reflectance time- matching our field data observations. \n",
    "\n",
    "The columns `total_pv, total_npc, total_bs` are calculated from the field-measured fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bs_fn(bs):\n",
    "#     '''takes bare soil result and scales and adds value. coefficients from: https://github.com/GeoscienceAustralia/fc/pull/48/files''' \n",
    "#     bs = (bs * 0.9499) + 2.45\n",
    "#     return bs\n",
    "\n",
    "# def pv_fn(pv):\n",
    "#     '''takes pv result and scales and adds value. coefficients from: https://github.com/GeoscienceAustralia/fc/pull/48/files''' \n",
    "#     pv = (pv * 0.9481) + 2.77\n",
    "#     return pv\n",
    "\n",
    "# def npv_fn(npv):\n",
    "#     '''takes npv result and scales and adds value. coefficients from: https://github.com/GeoscienceAustralia/fc/pull/48/files'''\n",
    "#     npv = (npv * 0.9578) - 0.73\n",
    "#     return npv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check with Emma that this is reasonable\n",
    "bs_fn(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create columns in the results 'field' table to calculate the transformed, new FC results\n",
    "\n",
    "# field['new_pv'] = field['pv'].apply(pv_fn)\n",
    "# field['new_npv'] = field['npv'].apply(npv_fn)\n",
    "# field['new_bs'] = field['bs'].apply(bs_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field.to_file('field_reprocessed_fc_%s.shp'%''.join(sensor_name.split()))\n",
    "# field = gpd.read_file('field_reprocessed_fc_%s.shp'%''.join(sensor_name.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(field_all, title=None):\n",
    "    field = field_all[(field_all[['new_pv','new_npv','new_bs']]>=0.).all(axis=1)]\n",
    "    field = field[(field[['pv_std','npv_std','bs_std']]<=10.).all(axis=1)] #HAVENT CHANGED THIS YET #FIXME\n",
    "    field = field[field['ue'] <25]\n",
    "    print(\"# of validation points:\", len(field))\n",
    "    \n",
    "    regr = linear_model.LinearRegression(fit_intercept=False)    \n",
    "\n",
    "    f = plt.figure(figsize=(12,12))\n",
    "    gs = gridspec.GridSpec(2,2)\n",
    "\n",
    "    xedges=yedges=list(np.arange(0,102,2))\n",
    "    X, Y = np.meshgrid(xedges, yedges)\n",
    "    cmname='YlGnBu'\n",
    "    if title: plt.suptitle(title)\n",
    "    \n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    field.plot(markersize=1, ax= ax1, color='r')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_title('Field Sites')\n",
    "    ax1.text(0.05, 0.05, \"%d points\"%len(field), transform=ax1.transAxes)\n",
    "    \n",
    "    rmses = []\n",
    "    for band_id, band in enumerate(['BS','PV','NPV']):\n",
    "        arr1 = field['total_%s'%band.lower()].values.ravel()*100.\n",
    "        arr2 = field[band.lower()].values.ravel()\n",
    "        regr.fit(arr1[:,np.newaxis], arr2[:,np.newaxis])\n",
    "        \n",
    "        print('Band:{0}, slope={1}, r2={2}'.format(band, regr.coef_[0][0],\n",
    "                                                regr.score(arr1[:,np.newaxis], arr2[:,np.newaxis])))\n",
    "        sr = spearmanr(arr1, arr2)[0]\n",
    "        print('Correlations:', pearsonr(arr1, arr2)[0], sr, kendalltau(arr1, arr2)[0])\n",
    "        rmse = np.sqrt(mean_squared_error(arr1, arr2))\n",
    "        print('RMSE:',rmse)\n",
    "        rmses.append(rmse)\n",
    "\n",
    "        ax1 = plt.subplot(gs[band_id+1])\n",
    "        ax1.scatter(arr1, arr2, s=3)\n",
    "        ax1.set_title(band)\n",
    "        \n",
    "        ax1.plot([0,100],[0,100])\n",
    "        ax1.plot(np.arange(0,100,10), regr.predict(np.arange(0,100,10)[:,np.newaxis]), ':')\n",
    "        ax1.text(5, 95, 'spearmanr = {0:.2f}'.format(sr))\n",
    "        ax1.text(5, 90, 'rmse = {0:.2f}'.format(rmse))\n",
    "        ax1.set_xlabel('Field Measured')\n",
    "        ax1.set_ylabel('%s FC'%sensor_name.upper())\n",
    "        ax1.set_xlim((0,100))\n",
    "        ax1.set_ylim((0,100))\n",
    "    \n",
    "    f.savefig('validate_reprocessed_%s.png'%''.join(sensor_name.split()))\n",
    "\n",
    "\n",
    "validate(field, title=sensor_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
